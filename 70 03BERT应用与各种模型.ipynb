{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[针对序列级和词元级应用程序微调BERT](http://zh.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参考资料[2020一文看尽各种 NLP 任务](https://zhuanlan.zhihu.com/p/163281686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们为自然语言处理应用设计了不同的模型，例如基于循环神经网络、卷积神经网络、注意力和多层感知机。这些模型在有空间或时间限制的情况下是有帮助的，但是，为每个自然语言处理任务精心设计一个特定的模型实际上是不可行的。在 14.8节中，我们介绍了一个名为BERT的预训练模型，该模型可以对广泛的自然语言处理任务进行最少的架构更改。一方面，在提出时，BERT改进了各种自然语言处理任务的技术水平。另一方面，正如在 14.10节中指出的那样，原始BERT模型的两个版本分别带有1.1亿和3.4亿个参数。因此，当有足够的计算资源时，我们可以考虑为下游自然语言处理应用微调BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 自然语言任务太多，每个模型都设计个网络太费劲了\n",
    "- 利用BERT预训练，能够使得其处理多种任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们将自然语言处理应用的子集概括为序列级和词元级。在序列层次上，介绍了在单文本分类任务和文本对分类（或回归）任务中，如何将文本输入的BERT表示转换为输出标签。在词元级别，我们将简要介绍新的应用，如文本标注和问答，并说明BERT如何表示它们的输入并转换为输出标签。在微调期间，不同应用之间的BERT所需的“最小架构更改”是额外的全连接层。在下游应用的监督学习期间，额外层的参数是从零开始学习的，而预训练BERT模型中的所有参数都是微调的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 自然语言按照任务的级别可以分为序列级别和词元级别\n",
    "- 一种是文本序列到文本序列，比如机器翻译，文本风格迁移等\n",
    "- 另一种是序列到类别，比如情感分类，实体命名识别，主题分类，槽位填充等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 类型1：单文本分类\n",
    "  - 单文本分类将单个文本序列作为输入，并输出其分类结果。 \n",
    "  - 除了我们在这一章中探讨的情感分析之外，语言可接受性语料库（Corpus of Linguistic Acceptability，COLA）也是一个单文本分类的数据集，它的要求判断给定的句子在语法上是否可以接受。 \n",
    "  - 例如，“I should study.”是可以接受的，但是“I should studying.”不是可以接受的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://zh.d2l.ai/_images/bert-one-seq.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 类型2 文本对分类或回归\n",
    "  - 以一对文本作为输入但输出连续值，语义文本相似度是一个流行的“文本对回归”任务。 这项任务评估句子的语义相似度。例如，在语义文本相似度基准数据集（Semantic Textual Similarity Benchmark）中，句子对的相似度得分是从0（无语义重叠）到5（语义等价）的分数区间 [Cer et al., 2017]。我们的目标是预测这些分数。来自语义文本相似性基准数据集的样本包括（句子1，句子2，相似性得分）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- “A plane is taking off.”（“一架飞机正在起飞。”），”An air plane is taking off.”（“一架飞机正在起飞。”），5.000分;\n",
    "\n",
    "- “A woman is eating something.”（“一个女人在吃东西。”），”A woman is eating meat.”（“一个女人在吃肉。”），3.000分;\n",
    "\n",
    "- “A woman is dancing.”（一个女人在跳舞。），”A man is talking.”（“一个人在说话。”），0.000分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <img src=\"http://zh.d2l.ai/_images/bert-two-seqs.svg\">\n",
    "- 文本对分类或回归应用程序的BERT微调，如自然语言推断和语义文本相似性（假设输入文本对分别有两个词元和三个词元）¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词元级任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 类型1： 文本标注\n",
    "- 考虑词元级任务，比如文本标注（text tagging），\n",
    "- 其中每个词元都被分配了一个标签。在文本标注任务中，词性标注为每个单词分配词性标记（例如，形容词和限定词）。 \n",
    "- 根据单词在句子中的作用。\n",
    "  - 如，在Penn树库II标注集中，句子“John Smith‘s car is new”应该被标记为“NNP（名词，专有单数）NNP POS（所有格结尾）NN（名词，单数或质量）VB（动词，基本形式）JJ（形容词）”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <img src=\"http://zh.d2l.ai/_images/bert-tagging.svg\">\n",
    "- 与上一个任务的唯一的区别在于，在文本标注中，输入文本的每个词元的BERT表示被送到相同的额外全连接层中，以输出词元的标签，例如词性标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 类型2： 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 问答反映阅读理解能力。 \n",
    "  - 例如，斯坦福问答数据集（Stanford Question Answering Dataset，SQuAD v1.1）由阅读段落和问题组成，其中每个问题的答案只是段落中的一段文本（文本片段） [Rajpurkar et al., 2016]。\n",
    "  - 举个例子，考虑一段话：\n",
    "  - “Some experts report that a mask’s efficacy is inconclusive.However,mask makers insist that their products,such as N95 respirator masks,can guard against the virus.”\n",
    "    - （“一些专家报告说面罩的功效是不确定的。然而，口罩制造商坚持他们的产品，如N95口罩，可以预防病毒。”）\n",
    "  - 还有一个问题“Who say that N95 respirator masks can guard against the virus?”\n",
    "    - （“谁说N95口罩可以预防病毒？”）。\n",
    "  - 答案应该是文章中的文本片段“mask makers”（“口罩制造商”）。\n",
    "  - 因此，SQuAD v1.1的目标是在给定问题和段落的情况下预测段落中文本片段的开始和结束。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://zh.d2l.ai/_images/bert-qa.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了微调BERT进行问答，在BERT的输入中，将问题和段落分别作为第一个和第二个文本序列。为了预测文本片段开始的位置，相同的额外的全连接层将把来自位置i的任何词元的BERT表示转换成标量分数$s_i$。文章中所有词元的分数还通过softmax转换成概率分布，从而为文章中的每个词元位置分配i作为文本片段开始的概率$p_i$。预测文本片段的结束与上面相同，只是其额外的全连接层中的参数与用于预测开始位置的参数无关。当预测结束时，位置i的词元由相同的全连接层变换成标量分数$e_i$。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
