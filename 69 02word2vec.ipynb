{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 直接就这视频往下看是看不懂的，我们有必要先进行一些准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入（Word2vec）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自然语言是用来表达人脑思维的复杂系统。 在这个系统中，词是意义的基本单元。\n",
    "- 顾名思义， 词向量是用于表示单词意义的向量， 并且还可以被认为是单词的特征向量或表示。 将单词映射到实向量的技术称为词嵌入。 近年来，词嵌入逐渐成为自然语言处理的基础知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为何独热向量是一个糟糕的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们曾经使用独热向量来表示词（字符就是单词）。假设词典中不同词的数量（词典大小）为，每个词对应一个从到的不同整数（索引）。\n",
    "- 为了得到索引为的任意词的独热向量表示，我们创建了一个全为0的长度为N的向量，并将位置i的元素设置为1。这样，每个词都被表示为一个长度为N的向量，可以直接由神经网络使用。\n",
    "- 虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度，\n",
    "- 比如我们经常使用的“余弦相似度”。对于向量，它们的余弦相似度是它们之间角度的余弦：\n",
    "- $$\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1].$$\n",
    "- 独热向量不能反映向量之间的相似性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自监督的word2vec\n",
    "- word2vec工具是为了解决上述问题而提出的。\n",
    "  - 它将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系。\n",
    "- word2vec工具包含两个模型，\n",
    "  - 即跳元模型（skip-gram） [Mikolov et al., 2013b](/%E9%99%84%E5%BD%9500%20%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf)和连续词袋（CBOW） [Mikolov et al., 2013a](/%E9%99%84%E5%BD%9500%20%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space.pdf)。\n",
    "  - 对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作是使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 跳元模型（Skip-Gram）\n",
    "- 跳元模型假设一个词可以用来在文本序列中生成其周围的单词。以文本序列“the”、“man”、“loves”、“his”、“son”为例。假设中心词选择“loves”，并将上下文窗口设置为2，如图下图所示，给定中心词“loves”，跳元模型考虑生成上下文词“the”、“man”、“him”、“son”的条件概率：\n",
    "  - $$P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "- 假设上下文词是在给定中心词的情况下独立生成的（即条件独立性）。在这种情况下，上述条件概率可以重写为：\n",
    "  - $$P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "\n",
    "\n",
    "- <img src = \"http://zh.d2l.ai/_images/skip-gram.svg\">\n",
    "- 跳元模型考虑了在给定中心词的情况下生成周围上下文词的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在跳元模型中，每个词都有两个d维向量表示，用于计算条件概率。\n",
    "- 更具体地说，对于词典中索引为i的任何词，分别用$\\mathbf{v}_i\\in\\mathbb{R}^d$和$\\mathbf{u}_i\\in\\mathbb{R}^d$表示其用作中心词和上下文词时的两个向量。\n",
    "- 给定中心词$w_c$（词典中的索引c），生成任何上下文词$w_o$（词典中的索引o）的条件概率可以通过对向量点积的softmax操作来建模：\n",
    "- $$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人理解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
