{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "- 我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。\n",
    "- 一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科的合集上预训练的，但它很难吸引这本书的大多数读者。\n",
    "- 另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。\n",
    "- 为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2。\n",
    "- 与用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。\n",
    "- 保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出文本的加载形式，文本被加载成列表的形式，每个列表又是一个储存了多个句子的列表.\n",
    "里面的为止部分已经替换成了unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['major changes to the routing of m @-@ 47 started in december 1960 when the i @-@ 75 / us 10 / us 23 freeway opened between saginaw and bay city',\n",
       " 'us 10 was rerouted east of midland to bay city along the m @-@ 20 freeway',\n",
       " 'm @-@ 47 was rerouted along the former us 10 from saginaw to east of midland using a connector expressway from freeland north to the us 10 freeway',\n",
       " 'm @-@ 81 was extended over state street in saginaw , and the former routing of m @-@ 47 between saginaw and bay city was redesignated as m @-@ 84',\n",
       " 'm @-@ 13 also replaced the former us 23 / m @-@ 47 when us 23 was moved to freeways as well',\n",
       " 'the southern end of m @-@ 47 was changed in 1962 with the completion of i @-@ 96 in the lansing area',\n",
       " 'us 16 was replaced by m @-@ 43 , and the southern terminus of m @-@ 47 was moved to exit 122 along i @-@ 96',\n",
       " 'this segment of m @-@ 47 south of m @-@ 46 became an extension of m @-@ 52 in 1969 , <unk> m @-@ 47 to hemlock',\n",
       " 'the interchange at <unk> road north of freeland opened in 1970 , and the expressway segment was upgraded to a full freeway',\n",
       " 'at the same time , m @-@ 47 was truncated to its current routing , resulting in the elimination of the m @-@ 46 / m @-@ 47 concurrency near shields',\n",
       " 'in the end , only about a mile and a half ( 2 @.@ 4 km ) of roadway still bears the m @-@ 47 from before the changes made starting in 1960 , along a section of road that was not originally part of the highway in 1919 .']"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "paragraphs = _read_wiki(data_dir)\n",
    "paragraphs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是bert所需的输入数据：\n",
    "- bert和以往模型不一样的地方在于，有非常多的输入数据\n",
    "- 1、和其他模型一样，语料库相关数据，\n",
    "- 2、segment，对于句子对进行前后句子区分\n",
    "- 3、句子有效长度\n",
    "- 4、掩码矩阵权重、掩码信息\n",
    "- 5、下一个句子预测训练样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_get_next_sentence函数生成二分类任务的训练样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    \"\"\"这个函数的目的就是在于生成二分类的任务\n",
    "    输入一个句子sentence,以及其真实的下一个句子\n",
    "    有一半概率, 我接下来的句子就是真实句子\n",
    "    但有有一半概率我把这个真实的next_sentence丢掉, \n",
    "    直接从paragraphs里面选择一个句子来作为next_sentence\n",
    "    那么我就构建好了这么一个预测模型:\n",
    "        如果next_sentence确实是,那么is_next就是True\n",
    "        反之is_next就是False, 所以is_next就是标签\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面的函数通过调用_get_next_sentence函数从输入paragraph生成用于下一句预测的训练样本。\n",
    "- 这里paragraph是句子列表，其中每个句子都是词元列表。\n",
    "- 自变量max_len指定预训练期间的BERT输入序列的最大长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        # 这里把每个句子\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        # 这里生成的tokens, segments就可以作为bert的输入了\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        # 这里是构建下一个句子预测的样本,这里可以看到放入的是tokens, segments,而不是token_a, token_b\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演示一下如何使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [d2l.tokenize(\n",
    "    paragraph, token='word') for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时paragraphs是将整个文章彻底拆开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['major',\n",
       " 'changes',\n",
       " 'to',\n",
       " 'the',\n",
       " 'routing',\n",
       " 'of',\n",
       " 'm',\n",
       " '@-@',\n",
       " '47',\n",
       " 'started',\n",
       " 'in',\n",
       " 'december',\n",
       " '1960',\n",
       " 'when',\n",
       " 'the',\n",
       " 'i',\n",
       " '@-@',\n",
       " '75',\n",
       " '/',\n",
       " 'us',\n",
       " '10',\n",
       " '/',\n",
       " 'us',\n",
       " '23',\n",
       " 'freeway',\n",
       " 'opened',\n",
       " 'between',\n",
       " 'saginaw',\n",
       " 'and',\n",
       " 'bay',\n",
       " 'city']"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [sentence for paragraph in paragraphs\n",
    "                for sentence in paragraph]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences是把所有句子全部拉成一个列表，不再区分paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us',\n",
       " '10',\n",
       " 'was',\n",
       " 'rerouted',\n",
       " 'east',\n",
       " 'of',\n",
       " 'midland',\n",
       " 'to',\n",
       " 'bay',\n",
       " 'city',\n",
       " 'along',\n",
       " 'the',\n",
       " 'm',\n",
       " '@-@',\n",
       " '20',\n",
       " 'freeway']"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs:\n",
    "    sample = _get_nsp_data_from_paragraph(paragraph,paragraphs,vocab,64)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，_get_nsp_data_from_paragraph将句子处理成了[[sentence]，[segment]]形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<cls>',\n",
       " 'by',\n",
       " 'the',\n",
       " '1890s',\n",
       " ',',\n",
       " 'the',\n",
       " 'observatory',\n",
       " 'had',\n",
       " 'become',\n",
       " 'crowded']"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][1][30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成遮蔽语言模型任务的数据\n",
    "- 为了从BERT输入序列生成遮蔽语言模型的训练样本，我们定义了以下_replace_mlm_tokens函数。\n",
    "- 在其输入中，tokens是表示BERT输入序列的词元的列表，candidate_pred_positions是不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及num_mlm_preds指示预测的数量（选择15%要预测的随机词元）。在 14.8.5.1节中\n",
    "- 定义遮蔽语言模型任务之后，在每个预测位置，输入可以由特殊的“掩码”词元或随机词元替换，或者保持不变。最后，该函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BERT预训练任务中被选中的15%的词中仅80%的词是真正被Masked掉的,另外10%不变，最后10%随机换为其他词，\n",
    "\"\"\"\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
    "                        vocab):\n",
    "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    \"\"\"\n",
    "    后面代码显示 num_mlm_preds = max(1, round(len(tokens) * 0.15)) \n",
    "    num_mlm_preds为整体token长度的15%，至少为1\n",
    "    \n",
    "    candidate_pred_positions是把tokens里面不是'<cls>', '<sep>'的词元索引拿出来\n",
    "    \n",
    "    \"\"\"\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # num_mlm_preds用于控制百分之多少的词用于替换\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        ##################################################\n",
    "        # 下面这部分是按比例去生成masked_token\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        #################################################    \n",
    "        #  mlm_pred_position是candidate_pred_positions中选出是索引\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    # mlm_input_tokens：token替换后的token_list, pred_positions_and_labels :待预测索引和标签\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'by', 'the', '1890s', ',', 'the', 'observatory', 'had', 'become', 'crowded', 'by', 'the', 'rapidly', 'growing', 'university', '<sep>', '\"', 'in', '2014', ',', 'jack', '<unk>', 'of', 'what', 'culture', 'ranked', 'him', 'as', 'the', 'second', 'greatest', 'ever', 'fighting', 'game', 'character', '.', '<sep>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "for tokens, segments, is_next in _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    print(tokens,end='\\n')\n",
    "    print(segments,end='\\n')\n",
    "    print(is_next,end='\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pred_positions = []\n",
    "# tokens是一个字符串列表\n",
    "for i, token in enumerate(tokens):\n",
    "    # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "    if token in ['<cls>', '<sep>']:\n",
    "        continue\n",
    "    candidate_pred_positions.append(i)\n",
    "num_mlm_preds = max(1, round(len(tokens) * 0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数演示,和上面原始的tokens对比，可以看到mlm_input_tokens已经把部分词替换成了mask，部分词进行了随机替换\n",
    "- pred_positions_and_labels储存的是你具体吧哪个位置进行了替换，替换前是啥样\n",
    "  - 比如(14, 'under')就是把第14个的'under'替换成了mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'by', 'the', '1890s', '<mask>', 'the', 'observatory', 'had', 'become', 'crowded', 'by', 'the', 'rapidly', 'growing', 'university', '<sep>', '\"', 'in', '2014', ',', 'jack', '<unk>', 'of', 'what', 'culture', 'ranked', '<mask>', 'as', 'the', '<mask>', 'greatest', 'ever', '<mask>', 'game', 'knee', '<mask>', '<sep>']\n",
      "[(35, '.'), (34, 'character'), (29, 'second'), (32, 'fighting'), (26, 'him'), (4, ',')]\n"
     ]
    }
   ],
   "source": [
    "mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens,candidate_pred_positions,num_mlm_preds,vocab)\n",
    "print(mlm_input_tokens)\n",
    "print(pred_positions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用前述的_replace_mlm_tokens函数，以下函数将BERT输入序列（tokens）作为输入，并返回输入词元的索引、发生预测的词元索引以及这些预测的标签索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    # tokens是一个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    # 输入词元的索引、发生预测的词元索引以及这些预测的标签索引。\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依旧是演示\n",
    "- vocab[mlm_input_tokens]返回的是vocab里面每个token的索引\n",
    "- pred_positions就是替换的位置\n",
    "- vocab[mlm_pred_labels]是具体替换的词在vocab的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ','),\n",
       " (26, 'him'),\n",
       " (29, 'second'),\n",
       " (32, 'fighting'),\n",
       " (34, 'character'),\n",
       " (35, '.')]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本转换为预训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT预训练定制一个Dataset类。在此之前，我们仍然需要定义辅助函数_pad_bert_inputs来将特殊的“<mask>”词元附加到输入。它的参数examples包含来自两个预训练任务的辅助函数_get_nsp_data_from_paragraph和_get_mlm_data_from_tokens的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    # max_len是用于控制单个sentence的长度的\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    # 这一段很明显是要吧单个example里面的全部数据进行一次整合，一个example对应一个paragraph的话，要整合到sentence的级别\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
    "         is_next) in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "            max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "            max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先看看example长啥样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for paragraph in paragraphs:\n",
    "    examples.extend(_get_nsp_data_from_paragraph(\n",
    "        paragraph, paragraphs, vocab, max_len))\n",
    "# 获取遮蔽语言模型任务的数据\n",
    "examples = [(_get_mlm_data_from_tokens(tokens, vocab)\n",
    "                + (segments, is_next))\n",
    "                for tokens, segments, is_next in examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_mlm_preds = round(max_len * 0.15)\n",
    "all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "nsp_labels = []\n",
    "for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
    "        is_next) in examples:\n",
    "    # 长度不够的话，用'<pad>'进行填充，当然这里只能返回填充的索引\n",
    "    all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))\n",
    "    # segments，长度不够填充为0\n",
    "    all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "    # 句子有效长度统计\n",
    "    valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "    # mask替换位置，因为规定是替换15%的token，但是具体到每个sentence，长度会不够，同样用0 补齐\n",
    "    all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "    # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "    all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),dtype=torch.float32))\n",
    "    \n",
    "    all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "    \n",
    "    nsp_labels.append(torch.tensor(is_next, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_token_ids储存了sentence里面的句子在vocab的索引，$'<pad>'$索引为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   3,   22,    2, 6320, 9883,    5, 3243,    2,  354, 8634,   22,    5,\n",
       "        1894, 1683,  311,    4,   37,    2,   13,  166,    2,    2,   35,  196,\n",
       "          10, 2494,   59, 1655, 2535,    6,    8,  666,   26,   13,  336,   10,\n",
       "          35,    5,   40,  731,  688,   17, 2494,   33,   32, 2535,    4,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_token_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47.)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  7, 17, 20, 21,  0,  0,  0])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred_positions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mlm_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   5,    6, 3243,   33, 1210,  454,   10,    0,    0,    0])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mlm_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到这里，我们总算可以对目前的处理进行总结，李沐大佬把目前的所有处理数据整合成了一个example，方便后续进行计算，一个example对应一个paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出一个标准example是包含了_get_nsp_data_from_paragraph和_get_mlm_data_from_tokens的所有东西，包括如下东西\n",
    "- _get_nsp_data_from_paragraph的返回：tokens, segments, is_next\n",
    "- _get_mlm_data_from_tokens 的返回：vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "- 也就是说包含如下信息\n",
    "  - tokens:句子对\n",
    "    - 注意下，本代码的定义中：tokens = ['<cls>'] + tokens_a + ['<sep>']， tokens是句子对而非vocab里面的tokens\n",
    "  - segments：句子对信息\n",
    "  - is_next：下一个句子训练样本\n",
    "  - vocab[mlm_input_tokens]：15%选出的token对于语料库里面的索引\n",
    "  - pred_positions：这15%token里面，被屏蔽或者被替换的索引位置（在tokens里面的索引）\n",
    "  - vocab[mlm_pred_labels]：这被替换的是哪些词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在理解前面的内容后，_WikiTextDataset没什么好说的，不过是把前面的东西整合一遍罢了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # 获取遮蔽语言模型任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "            examples, max_len, self.vocab)\n",
    "         \n",
    "    # 记住这个返回的序列即可。\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是吧Dataset，转变成train_iter即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers() # 4\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                        shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,     5,  2738,     2,    46,  1933,     9,     5,  1630,   137,\n",
       "             2,    11,   923,    20,     5,     2,   256,   976,  8344,    21,\n",
       "          1142, 16889,     8,  5576,     2,     2,   532,     4,     5,  1771,\n",
       "             0,  1880,     0,    23, 13297,   619,     7,    42,     7,     5,\n",
       "            75,   643,  5576,    57,  1493,    11,  3626,    86,    18,     4,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1],\n",
       "        [    3,     2,     5,  2085,     6,     2,  9117,     7,    11,   412,\n",
       "            14,  1544, 14825,    31,    98,    10,     0,     5,     0,     8,\n",
       "          9332,     2,  2084,    14,  6591,    14,  2084,     2,  4430,     0,\n",
       "            21,  1544,     4,     0,     0,  7054,    98,    16,    11,   975,\n",
       "            20,   662,    14, 11487, 14143,    18,     4,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_X[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment编码，表示是第几个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_X[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个句子有效长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50., 47., 51., 34., 59., 53., 44., 51., 59., 64., 42., 55., 21., 41.,\n",
       "        48., 58., 57., 49., 24., 46., 54., 50., 32., 48., 31., 47., 46., 53.,\n",
       "        58., 24., 49., 41., 40., 38., 46., 64., 42., 47., 54., 26., 57., 60.,\n",
       "        54., 23., 53., 29., 53., 41., 59., 43., 64., 44., 52., 54., 41., 43.,\n",
       "        27., 64., 40., 57., 57., 43., 39., 54., 43., 43., 44., 50., 46., 52.,\n",
       "        56., 50., 39., 57., 36., 45., 38., 37., 41., 42., 49., 39., 25., 53.,\n",
       "        44., 46., 37., 49., 46., 40., 61., 57., 51., 37., 52., 50., 48., 37.,\n",
       "        47., 43.])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lens_x[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待预测句子的位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 10, 12, 18, 20, 32, 36, 41,  0,  0],\n",
       "        [ 7, 10, 20,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2, 12, 14, 22, 26,  0,  0,  0,  0,  0],\n",
       "        [ 4,  7,  9, 30, 33, 34,  0,  0,  0,  0],\n",
       "        [ 2,  9, 16, 26, 31, 33,  0,  0,  0,  0],\n",
       "        [ 2,  3, 12, 14, 19, 21,  0,  0,  0,  0],\n",
       "        [ 4,  5,  6, 17, 24, 30, 37, 42, 48,  0],\n",
       "        [ 9, 20, 22, 26, 27, 29, 31, 35,  0,  0],\n",
       "        [ 5, 13, 14,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 4,  6,  7, 10, 13, 19, 21,  0,  0,  0]])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_positions_X[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩码矩阵初试权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_weights_X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掩码位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,     5,    16,   161,     6,   164,     5,    32,     0,     0],\n",
       "        [    9,  6101,     7,  6591, 14825,  4430,    23,     0,     0,     0],\n",
       "        [  298,    43,   169,  5899,   402,  2542, 10192,  3651,     0,     0],\n",
       "        [  223,   947,    51,   157,   482,     0,     0,     0,     0,     0],\n",
       "        [  197,  2678,    66,  1090,     0,  1649,   242,   466,  4434,     0],\n",
       "        [   13,     0,     6,   326,    16,    11,   895,    17,     0,     0],\n",
       "        [    5,  5252,     5,  3115,    12,     6,   904,     0,     0,     0],\n",
       "        [   14,   124,     7,     0,   181,   102,     5,     9,     0,     0],\n",
       "        [  991,   161, 10824,    20,     8,    43,    31,     0,  2196,     0],\n",
       "        [ 1057,   605,  8534,   414,    28,     8,  2777,    17,     0,  2499]])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
