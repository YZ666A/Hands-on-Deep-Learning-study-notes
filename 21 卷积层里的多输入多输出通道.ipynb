{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[21 卷积层里的多输入多输出通道](https://www.bilibili.com/video/BV1MB4y1F7of?spm_id_from=333.999.0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单输出通道模式\n",
    "- 每个通道有自己的卷积核，卷积结果为每个卷积核卷积后相加\n",
    "<img src=\"picture\\屏幕截图 2022-05-04 234542.png\"></a>\n",
    "\n",
    "### 多输出通道模式\n",
    "- 所谓多通道输出，其实可以理解为用不同的卷积核分别进行单通道输出，再叠加即可\n",
    "- c:通道数 h w为图像高和宽\n",
    "- 卷积核也相应的变成了三维\n",
    "<img src=\"picture\\屏幕截图 2022-05-04 235112.png\"></a>\n",
    "\n",
    "### 二维卷积层\n",
    "- 输入：通道数 X 图片高 X 图片宽\n",
    "- 核: 输出通道数 X 输入通道数 X 图片高 X 图片宽\n",
    "- 偏差：输出通道数 X 输入通道数\n",
    "- 输出Y: 输出通道数 X 卷积高 X 卷积宽\n",
    "<img src=\"picture\\屏幕截图 2022-05-05 005648.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 单输入通道的代码实现\n",
    "def corr2d_multi_in(X,K):\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[11, 22, 33],\n",
      "        [44, 55, 66],\n",
      "        [77, 88, 99]])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[11, 22, 33],\n",
      "         [44, 55, 66],\n",
      "         [77, 88, 99]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = torch.tensor([[11, 22, 33], [44, 55, 66], [77, 88, 99]])\n",
    "c = torch.stack([a, b], 0)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多输出通道的代码实现\n",
    "def corr2d_multi_in_out(X, K):\n",
    "# 迭代“K”的第0个维度，每次都对输⼊“X”执⾏互相关运算。\n",
    "# 最后将所有结果都叠加在⼀起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "# print(K)\n",
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 证明1*1卷积等价于全连接\n",
    "# 这里生成一个全连接层\n",
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    # c_i,c_o 为输入输出通道数\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    # 这里是全连接层将矩阵进行展平操作，每个通道都进行展开\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    # 全连接层中的矩阵乘法\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.2823, -0.8331, -0.8059],\n",
       "          [-0.7143, -0.5921,  1.0605],\n",
       "          [-1.2789, -0.5802,  0.0334]],\n",
       " \n",
       "         [[ 0.2139,  0.3961, -0.1117],\n",
       "          [ 0.6905, -1.4285, -0.3292],\n",
       "          [ 2.1695,  0.3954, -1.7959]],\n",
       " \n",
       "         [[ 2.0091,  1.7758, -0.0570],\n",
       "          [ 1.1330,  0.6528,  1.0920],\n",
       "          [-0.9390,  1.3047, -0.2772]]]),\n",
       " tensor([[[[ 0.7333]],\n",
       " \n",
       "          [[-0.9500]],\n",
       " \n",
       "          [[-0.4692]]],\n",
       " \n",
       " \n",
       "         [[[ 0.5505]],\n",
       " \n",
       "          [[ 0.2558]],\n",
       " \n",
       "          [[-0.1276]]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X,K为正态分布的向量\n",
    "X = torch.normal(0, 1, (3, 3, 3)) \n",
    "# 这里K为卷积核，这里显然使用了两个不同卷积核作为输出\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "X,K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.8195, -1.8205, -0.4582],\n",
      "         [-1.7115,  0.6166,  0.5781],\n",
      "         [-2.5583, -1.4133,  1.8607]],\n",
      "\n",
      "        [[-1.4581, -0.5839, -0.4650],\n",
      "         [-0.3612, -0.7747,  0.3602],\n",
      "         [-0.0292, -0.3848, -0.4056]]])\n",
      "tensor([[[-2.8195, -1.8205, -0.4582],\n",
      "         [-1.7115,  0.6166,  0.5781],\n",
      "         [-2.5583, -1.4133,  1.8607]],\n",
      "\n",
      "        [[-1.4581, -0.5839, -0.4650],\n",
      "         [-0.3612, -0.7747,  0.3602],\n",
      "         [-0.0292, -0.3848, -0.4056]]])\n"
     ]
    }
   ],
   "source": [
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q/A\n",
    "- Q22:每个通道的卷积核的值是不一样的，但是为了计算方便，卷积核大小是一样的\n",
    "- Q24:卷积核也是通过学习给出的\n",
    "- Q27:神经网络并不需要对数据进行过多的拆分（例如将数据分为高低频）\n",
    "- Q29:可以将每个通道分别用卷积核进行卷积，在利用一个(1,1,n)卷积核进行全连接，这么做的好处是计算量大幅度降低（使得手机计算神经网络成为可能），但相应的计算效果会差很多"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
