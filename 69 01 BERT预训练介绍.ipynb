{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-07 002650.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 大型网络可以采用迁移学习的方式部署，目前来看就是之前的微调概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-07 003501.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 之前的微调是用于cv，那么nlp能不能也这么干？\n",
    "- 而且和微调不一样的在于，特征抽取部分完全不训练会怎么样？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-07 004306.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有编码器的transformer，其实最麻烦是就是decoderblock了。。。但是整个这个貌似不要了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-08 010231.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert创新点\n",
    "- 由于没有解码器，因此直接将输入以句子对的形式输入\n",
    "- 句子对以$<cls>$开头，中间用$<seq>$隔开,每个句子对分别进入自己的编码器\n",
    "- 位置编码现在可以自己学习了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-10 185309.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 相较于机器翻译，bert更倾向于去实现一个更加通用的模型\n",
    "- bert只有编码器，编码器是双向的，为了避免看到未来的结果，就必须引入和之前soft_mask类似功能的函数\n",
    "- 用一定概率对模型进行掩码操作\n",
    "- 但是因为实际预测的时候，是不存在mask这个东西的，因此Bert必然会对mask标记进行转换，但是问题在于，如果迁移到别的任务上面去，就不能全部预测（这里是这么解释的，但我不是非常理解）\n",
    "- 转变成一定概率去预测mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/屏幕截图 2022-07-10 202916.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个任务是让模型去判断两个句子是否是相邻句子\n",
    "- 输入的时候，就会50%的概率来进行随机调整\n",
    "- 依旧不是很懂这一步的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 总的来说，Bert为了能够进行迁移学习，专门做了一些优化\n",
    "  - 采用了带掩码的语言模型\n",
    "  - 增加了下一个句子的预测\n",
    "- 创新点\n",
    "  - 模型更大，数据更多\n",
    "  - 只有解码器\n",
    "  - 输入句子对进行学习\n",
    "  - 位置编码可学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：bert所需词汇量非常大，10亿起步，和后来的GPT-3一样，也是现在深度学习变成军备竞赛的起点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
