{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数大概解释：一个保存了固定字典和大小的简单查找表。这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。相当于随机生成了一个tensor，可以把它看作一个查询表，其size为[num_embeddings，embedding_dim] 。其中num_embeddings是查询表的大小，embedding_dim是每个查询向量的维度，这里看起来很抽象，可以看下面的示例来进行理解。\n",
    "\n",
    "这是一个矩阵类，里面初始化了一个随机矩阵，矩阵的长是字典的大小，宽是用来表示字典中每个元素的属性向量，向量的维度根据你想要表示的元素的复杂度而定。类实例化之后可以根据字典中元素的下标来查找元素对应的向量。输入下标0，输出就是embeds矩阵中第0行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "nn.Embedding(num_embeddings: int, \n",
    "             embedding_dim: int, \n",
    "             padding_idx: Optional[int] = None,\n",
    "             max_norm: Optional[float] = None, \n",
    "             norm_type: float = 2., \n",
    "             scale_grad_by_freq: bool = False,\n",
    "             sparse: bool = False, \n",
    "             _weight: Optional[Tensor] = None,\n",
    "             device=None, dtype=None) -> None:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 一个简单的查找表，用于存储固定字典和大小的嵌入。\n",
    "\n",
    "- 该模块通常用于存储词嵌入并使用索引检索它们。\n",
    "     模块的输入是索引列表，输出是对应的\n",
    "     词嵌入。\n",
    "        \n",
    "    - 参数：\n",
    "        - num_embeddings (int): 嵌入字典的大小\n",
    "        - embedding_dim (int)：每个嵌入向量的大小\n",
    "        - padding_idx (int, optional): 如果指定，:attr:`padding_idx` 处的条目不会对梯度产生影响；\n",
    "        因此，:attr:`padding_idx` 处的嵌入向量在训练期间不会更新，\n",
    "        即它仍然是一个固定的“垫”。对于一个新建的 Embedding，\n",
    "        padding_idx 处的嵌入向量将默认为全零，\n",
    "        但可以更新为另一个值以用作填充向量。\n",
    "        - max_norm (float, optional): 如果给定，每个嵌入向量的范数大于:attr:`max_norm`\n",
    "        被重新规范化为具有范数:attr:`max_norm`。\n",
    "        - norm_type (float, optional): 为 :attr:`max_norm` 选项计算的 p-norm 的 p。默认“2”。\n",
    "        - scale_grad_by_freq (boolean, optional): 如果给定，这将通过频率的倒数来缩放梯度\n",
    "        小批量中的单词。默认“假”。\n",
    "        - sparse (bool, optional): If ``True``, 梯度 w.r.t. :attr:`weight` 矩阵将是一个稀疏张量。\n",
    "        有关稀疏渐变的更多详细信息，请参阅注释。\n",
    "    \n",
    "    - 属性：\n",
    "         - 权重（Tensor）：形状模块的可学习权重（num_embeddings，embedding_dim）\n",
    "         从 :math:$\\mathcal{N}(0, 1)$ 初始化\n",
    "\n",
    "\n",
    "    - 形状：\n",
    "         - 输入：:math:`(*)`、IntTensor 或 LongTensor 任意形状，包含要提取的索引\n",
    "         - 输出：:math:`(*, H)`，其中 `*` 是输入形状，:math:$H=\\text{embedding\\_dim}$\n",
    "\n",
    "    - 注意：\n",
    "         - 请记住，只有有限数量的优化器支持\n",
    "         稀疏梯度：目前是 :class:`optim.SGD`（`CUDA` 和 `CPU`），\n",
    "         :class:`optim.SparseAdam` (`CUDA` 和 `CPU`) 和 :class:`optim.Adagrad` (`CPU`)\n",
    "\n",
    "    -  注意:\n",
    "          - 当 :attr:`max_norm` 不是 `None` 时，:class:`Embedding` 的 forward 方法会修改\n",
    "                   :attr:`weight` 原位张量。 由于梯度计算所需的张量不能\n",
    "                   就地修改，在之前对“Embedding.weight”执行可微操作\n",
    "                   调用 Embedding 的 forward 方法需要克隆 Embedding.weight 时\n",
    "                   :attr:`max_norm` 不是“无”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例一：创建查询矩阵并使用它做Embedding："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2102,  0.6403,  1.9413],\n",
      "        [-0.1649,  0.6185, -0.7571],\n",
      "        [-0.3988, -0.2256, -0.9698],\n",
      "        [-0.5868, -1.6040,  0.3543],\n",
      "        [-1.4321,  0.0282, -1.4700]], requires_grad=True)\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[-0.2102,  0.6403,  1.9413],\n",
      "         [-0.3988, -0.2256, -0.9698],\n",
      "         [-0.2102,  0.6403,  1.9413],\n",
      "         [-0.1649,  0.6185, -0.7571]],\n",
      "\n",
      "        [[-0.1649,  0.6185, -0.7571],\n",
      "         [-0.5868, -1.6040,  0.3543],\n",
      "         [-1.4321,  0.0282, -1.4700],\n",
      "         [-1.4321,  0.0282, -1.4700]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(5, 3)  # 定义一个具有5个单词，维度为3的查询矩阵\n",
    "print(embedding.weight)  # 展示该矩阵的具体内容\n",
    "test = torch.LongTensor([[0, 2, 0, 1],\n",
    "                         [1, 3, 4, 4]])  # 该test矩阵用于被embed，其size为[2, 4]\n",
    "# 其中的第一行为[0, 2, 0, 1]，表示获取查询矩阵中ID为0, 2, 0, 1的查询向量\n",
    "# 可以在之后的test输出中与embed的输出进行比较\n",
    "test = embedding(test)\n",
    "print(test.size())  # 输出embed后test的size，为[2, 4, 3]，增加的3，是因为查询向量的维度为3\n",
    "print(test)  # 输出embed后的test的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出创建了一个具有5个ID(可以理解为拥有5个词的词典)的查询矩阵，每个查询向量的维度是3维，然后用一个自己需要Embedding的矩阵与之计算，其中的内容就是需要匹配的ID号，注意！如果需要Embedding的矩阵中的查询向量不为1，2这种整数，而是1.1这种浮点数，就不能与查询向量成功匹配，会报错，且如果矩阵中的值大于了查询矩阵的范围，比如这里是5，也会报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例二：寻找查询矩阵中特定ID(词)的查询向量(词向量)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5868, -1.6040,  0.3543]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问某个ID，即第N个词的查询向量(词向量)\n",
    "embedding(torch.LongTensor([3]))  # 这里表示查询第3个词的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*实例三：输出的hello这个词的word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3442, -0.7834,  2.0002, -2.0970, -0.5910]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "word_to_ix = {'hello': 0, 'world': 1}\n",
    "embeds = nn.Embedding(2, 5)\n",
    "hello_idx = torch.LongTensor([word_to_ix['hello']])\n",
    "hello_idx = Variable(hello_idx)\n",
    "hello_embed = embeds(hello_idx)\n",
    "hello_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_embeddings, embedding_dim, max_norm\n",
    "n, d, m = 3, 5, 7\n",
    "embedding = nn.Embedding(n, d, max_norm=True) \n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3583, -0.7382,  0.2429],\n",
       "         [ 0.3089,  1.8333,  2.1872],\n",
       "         [-1.6570,  0.6716, -1.1453],\n",
       "         [-0.2945,  1.8201, -0.4705]],\n",
       "\n",
       "        [[-1.6570,  0.6716, -1.1453],\n",
       "         [ 0.3928, -0.9647,  1.0261],\n",
       "         [ 0.3089,  1.8333,  2.1872],\n",
       "         [ 0.1122, -0.6475,  1.0859]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.6057, -1.7844, -0.8746],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-1.1813, -0.2832,  1.9689]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [-1.4273, -0.7405,  0.1696],\n",
       "        [ 0.6057, -1.7844, -0.8746],\n",
       "        [-1.4139, -1.1599,  2.3117],\n",
       "        [-0.0528, -0.1413,  1.0461],\n",
       "        [-1.1813, -0.2832,  1.9689],\n",
       "        [ 0.1231, -0.4302,  3.2592],\n",
       "        [ 0.8762,  1.2861, -1.5752],\n",
       "        [ 0.9645,  0.7281, -1.8929],\n",
       "        [ 1.6235, -0.8477,  0.6434]], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of changing `pad` vector\n",
    "padding_idx = 0\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [-1.4273, -0.7405,  0.1696],\n",
       "        [ 0.6057, -1.7844, -0.8746],\n",
       "        [-1.4139, -1.1599,  2.3117],\n",
       "        [-0.0528, -0.1413,  1.0461],\n",
       "        [-1.1813, -0.2832,  1.9689],\n",
       "        [ 0.1231, -0.4302,  3.2592],\n",
       "        [ 0.8762,  1.2861, -1.5752],\n",
       "        [ 0.9645,  0.7281, -1.8929],\n",
       "        [ 1.6235, -0.8477,  0.6434]], requires_grad=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embedding.weight[padding_idx] = torch.ones(3)\n",
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7aa0bfac01ce1e861177f7ddac38fea2b037d61c5b30928011344cb8e12ea48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
