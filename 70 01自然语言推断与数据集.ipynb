{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[70 自然语言推断与数据集](http://zh.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然语言推断（natural language inference）\n",
    "- 主要研究 假设（hypothesis）是否可以从前提（premise）中推断出来， 其中两者都是文本序列。 \n",
    "- 换言之，自然语言推断决定了一对文本序列之间的逻辑关系。这类关系通常分为三种类型：\n",
    "  - 蕴涵（entailment）：假设可以从前提中推断出来。\n",
    "\n",
    "  - 矛盾（contradiction）：假设的否定可以从前提中推断出来。\n",
    "\n",
    "  - 中性（neutral）：所有其他情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自然语言推断也被称为识别文本蕴涵任务。 \n",
    "- 例如，下面的一个文本对将被贴上“蕴涵”的标签，因为假设中的“表白”可以从前提中的“拥抱”中推断出来。\n",
    "  - 前提：两个女人拥抱在一起。\n",
    "\n",
    "  - 假设：两个女人在示爱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面是一个“矛盾”的例子，因为“运行编码示例”表示“不睡觉”，而不是“睡觉”。\n",
    "\n",
    "  - 前提：一名男子正在运行Dive Into Deep Learning的编码示例。\n",
    "\n",
    "  - 假设：该男子正在睡觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第三个例子显示了一种“中性”关系，因为“正在为我们表演”这一事实无法推断出“出名”或“不出名”。\n",
    "\n",
    "  - 前提：音乐家们正在为我们表演。\n",
    "\n",
    "  - 假设：音乐家很有名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言推断一直是理解自然语言的中心话题。它有着广泛的应用，从信息检索到开放领域的问答。为了研究这个问题，我们将首先研究一个流行的自然语言推断基准数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斯坦福自然语言推断（SNLI）数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "斯坦福自然语言推断语料库（Stanford Natural Language Inference，SNLI）是由500000多个带标签的英语句子对组成的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的SNLI数据集包含的信息比我们在实验中真正需要的信息丰富得多。因此，我们定义函数read_snli以仅提取数据集的一部分，然后返回前提、假设及其标签的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    def extract_text(s):\n",
    "        # 删除我们不会使用的信息\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # 用一个空格替换两个或多个连续的空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        # 移除头尾字符\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]] # \\t相当于tab按键\n",
    "    \n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_snli(data_dir, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“0”、“1”和“2”分别对应于“蕴涵”、“矛盾”和“中性”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n",
      "前提： Children smiling and waving at camera\n",
      "假设： They are smiling at their parents\n",
      "标签： 2\n",
      "前提： Children smiling and waving at camera\n",
      "假设： There are children present\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "for x0, x1, y in zip(train_data[0][:5], train_data[1][:5], train_data[2][:5]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集约有550000对，测试集约有10000对。下面显示了训练集和测试集中的三个标签“蕴涵”、“矛盾”和“中性”是平衡的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, is_train=False)\n",
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义用于加载数据集的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义一个用于加载SNLI数据集的类。\n",
    "- 类构造函数中的变量num_steps指定文本序列的长度，使得每个小批量序列将具有相同的形状。\n",
    "- 换句话说，在较长序列中的前num_steps个标记之后的标记被截断，\n",
    "- 而特殊标记“$<pad>$”将被附加到较短的序列后，直到它们的长度变为num_steps。这个和之前处理是一样的\n",
    "- 通过实现__getitem__功能，我们可以任意访问带有索引idx的前提、假设和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        # 按空格进行词元化\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        \n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
    "                                   min_freq=5,\n",
    "                                   reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([\n",
    "            d2l.truncate_pad(self.vocab[line], self.num_steps,\n",
    "                             self.vocab['<pad>']) for line in lines\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以调用read_snli函数和SNLIDataset类来下载SNLI数据集，并返回训练集和测试集的DataLoader实例，以及训练集的词表。值得注意的是，我们必须使用从训练集构造的词表作为测试集的词表。因此，在训练集中训练的模型将不知道来自测试集的任何新词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db1be6f39172faa660bce7f133330141f4fce14b9a8c52849f571eb71f2e9d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
