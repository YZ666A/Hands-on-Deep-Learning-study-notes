{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[66 使用注意力机制的seq2seq](https://www.bilibili.com/video/BV1v44y1C7Tg?spm_id_from=333.999.0.0&vd_source=1d3a7b81d826789081d8b6870d4fff8e)\n",
    "<img src=\"picture/截屏2022-06-22 20.15.52.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沐神解释了半天:总归原因可能就是以下几点\n",
    "1. 从翻译的角度来说, 单词与单词是一一对应的, 我们人类翻译的时候, 一般是一个单词一个单词翻译, 同时会结合上下文的语气等等\n",
    "2. 但是对于之前是seq2seq来说,  模型不会去关注某个重点单词, 输入整体被转换成隐藏层了, 实际上作为真正输入的只有最后一个单词(不是特别理解这句话,但大概是这么个意思)\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture/截屏2022-06-22 20.38.38.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 个人理解, 之前的encoder的隐藏层和输出层整体都作为输入传输给了decoder, 但这样会导致新的问题, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7aa0bfac01ce1e861177f7ddac38fea2b037d61c5b30928011344cb8e12ea48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
